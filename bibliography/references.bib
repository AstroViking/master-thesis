@ARTICLE{Dubey2019-yr,
  title         = "diffGrad: An Optimization Method for Convolutional Neural
                   Networks",
  author        = "Dubey, Shiv Ram and Chakraborty, Soumendu and Roy, Swalpa
                   Kumar and Mukherjee, Snehasis and Singh, Satish Kumar and
                   Chaudhuri, Bidyut Baran",
  abstract      = "Stochastic Gradient Decent (SGD) is one of the core
                   techniques behind the success of deep neural networks. The
                   gradient provides information on the direction in which a
                   function has the steepest rate of change. The main problem
                   with basic SGD is to change by equal sized steps for all
                   parameters, irrespective of gradient behavior. Hence, an
                   efficient way of deep network optimization is to make
                   adaptive step sizes for each parameter. Recently, several
                   attempts have been made to improve gradient descent methods
                   such as AdaGrad, AdaDelta, RMSProp and Adam. These methods
                   rely on the square roots of exponential moving averages of
                   squared past gradients. Thus, these methods do not take
                   advantage of local change in gradients. In this paper, a
                   novel optimizer is proposed based on the difference between
                   the present and the immediate past gradient (i.e.,
                   diffGrad). In the proposed diffGrad optimization technique,
                   the step size is adjusted for each parameter in such a way
                   that it should have a larger step size for faster gradient
                   changing parameters and a lower step size for lower gradient
                   changing parameters. The convergence analysis is done using
                   the regret bound approach of online learning framework.
                   Rigorous analysis is made in this paper over three synthetic
                   complex non-convex functions. The image categorization
                   experiments are also conducted over the CIFAR10 and CIFAR100
                   datasets to observe the performance of diffGrad with respect
                   to the state-of-the-art optimizers such as SGDM, AdaGrad,
                   AdaDelta, RMSProp, AMSGrad, and Adam. The residual unit
                   (ResNet) based Convolutional Neural Networks (CNN)
                   architecture is used in the experiments. The experiments
                   show that diffGrad outperforms other optimizers. Also, we
                   show that diffGrad performs uniformly well for training CNN
                   using different activation functions. The source code is
                   made publicly available at
                   https://github.com/shivram1987/diffGrad.",
  month         =  sep,
  year          =  2019,
  keywords      = "Master thesis",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1909.11015"
}

@ARTICLE{Xiao2018-ve,
  title         = "Dynamical Isometry and a Mean Field Theory of {CNNs}: How to
                   Train 10,000-Layer Vanilla Convolutional Neural Networks",
  author        = "Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha
                   and Schoenholz, Samuel S and Pennington, Jeffrey",
  abstract      = "In recent years, state-of-the-art methods in computer vision
                   have utilized increasingly deep convolutional neural network
                   architectures (CNNs), with some of the most successful
                   models employing hundreds or even thousands of layers. A
                   variety of pathologies such as vanishing/exploding gradients
                   make training such deep networks challenging. While residual
                   connections and batch normalization do enable training at
                   these depths, it has remained unclear whether such
                   specialized architecture designs are truly necessary to
                   train deep CNNs. In this work, we demonstrate that it is
                   possible to train vanilla CNNs with ten thousand layers or
                   more simply by using an appropriate initialization scheme.
                   We derive this initialization scheme theoretically by
                   developing a mean field theory for signal propagation and by
                   characterizing the conditions for dynamical isometry, the
                   equilibration of singular values of the input-output
                   Jacobian matrix. These conditions require that the
                   convolution operator be an orthogonal transformation in the
                   sense that it is norm-preserving. We present an algorithm
                   for generating such random initial orthogonal convolution
                   kernels and demonstrate empirically that they enable
                   efficient training of extremely deep architectures.",
  month         =  jun,
  year          =  2018,
  keywords      = "Master thesis",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1806.05393"
}

@MISC{Cowan2014-ya,
  title    = "{Self-Organized} Criticality and {Near-Criticality} in Neural
              Networks",
  author   = "Cowan, Jack D and Neuman, Jeremy and van Drongelen, Wim",
  journal  = "Criticality in Neural Systems",
  pages    = "465--484",
  year     =  2014,
  keywords = "Master thesis"
}

@INPROCEEDINGS{Divis2022-gj,
  title     = "Neural Criticality Metric for Object Detection Deep Neural
               Networks",
  booktitle = "Computer Safety, Reliability, and Security. {SAFECOMP} 2022
               Workshops",
  author    = "Divi{\v s}, V{\'a}clav and Schuster, Tobias and Hr{\'u}z, Marek",
  abstract  = "The complexity of state-of-the-art Deep Neural Network (DNN)
               architectures exacerbates the search for safety relevant metrics
               and methods that could be used for functional safety
               assessments. In this article, we investigate Neurons'
               Criticality (the ability to affect the decision process) for
               several object detection DNN architectures. As a first step, we
               introduce the Neural Criticality metric for object detection
               DNNs and set a theoretical background. Subsequently, by
               conducting experiments, we verify that removing one neuron from
               the computational graph of a DNN can have a significant
               (positive, as well as negative) influence on the prediction's
               precision (object classification and localization). Finally, we
               build statistics for each neuron from pre-trained networks on
               the COCO object detection validation dataset and examine the
               network stability for the most critical neurons in order to
               prove our metric's validity.",
  publisher = "Springer International Publishing",
  pages     = "276--288",
  year      =  2022,
  keywords  = "Master thesis"
}

@ARTICLE{Katsnelson2021-vx,
  title         = "Self-organized criticality in neural networks",
  author        = "Katsnelson, Mikhail I and Vanchurin, Vitaly and Westerhout,
                   Tom",
  abstract      = "We demonstrate, both analytically and numerically, that
                   learning dynamics of neural networks is generically
                   attracted towards a self-organized critical state. The
                   effect can be modeled with quartic interactions between
                   non-trainable variables (e.g. states of neurons) and
                   trainable variables (e.g. weight matrix). Non-trainable
                   variables are rapidly driven towards stochastic equilibrium
                   and trainable variables are slowly driven towards learning
                   equilibrium described by a scale-invariant distribution on a
                   wide range of scales. Our results suggest that the scale
                   invariance observed in many physical and biological systems
                   might be due to some kind of learning dynamics and support
                   the claim that the universe might be a neural network.",
  month         =  jul,
  year          =  2021,
  keywords      = "Master thesis",
  archivePrefix = "arXiv",
  primaryClass  = "cond-mat.stat-mech",
  eprint        = "2107.03402"
}

@ARTICLE{Chatterji2019-ix,
  title         = "The intriguing role of module criticality in the
                   generalization of deep networks",
  author        = "Chatterji, Niladri S and Neyshabur, Behnam and Sedghi, Hanie",
  abstract      = "We study the phenomenon that some modules of deep neural
                   networks (DNNs) are more critical than others. Meaning that
                   rewinding their parameter values back to initialization,
                   while keeping other modules fixed at the trained parameters,
                   results in a large drop in the network's performance. Our
                   analysis reveals interesting properties of the loss
                   landscape which leads us to propose a complexity measure,
                   called module criticality, based on the shape of the valleys
                   that connects the initial and final values of the module
                   parameters. We formulate how generalization relates to the
                   module criticality, and show that this measure is able to
                   explain the superior generalization performance of some
                   architectures over others, whereas earlier measures fail to
                   do so.",
  month         =  dec,
  year          =  2019,
  keywords      = "Master thesis",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1912.00528"
}

@ARTICLE{Knuth1984-ih,
  title     = "Literate Programming",
  author    = "Knuth, D E",
  abstract  = "Abstract. The author and his associates have been experimenting
               for the past several years with a programming language and
               documentation system called WEB. This",
  journal   = "Comput. J.",
  publisher = "Oxford Academic",
  volume    =  27,
  number    =  2,
  pages     = "97--111",
  month     =  jan,
  year      =  1984,
  keywords  = "Master thesis",
  language  = "en"
}

@ARTICLE{Sompolinsky1988-bv,
  title    = "Chaos in random neural networks",
  author   = "Sompolinsky, H and Crisanti, A and Sommers, H J",
  journal  = "Phys. Rev. Lett.",
  volume   =  61,
  number   =  3,
  pages    = "259--262",
  month    =  jul,
  year     =  1988,
  keywords = "Master thesis",
  language = "en"
}

@ARTICLE{Hoyer2017-nd,
  title     = "xarray: {N-D} labeled Arrays and Datasets in Python",
  author    = "Hoyer, Stephan and Hamman, Joe",
  abstract  = "xarray is an open source project and Python package that
               provides a toolkit and data structures for N-dimensional labeled
               arrays. Our approach combines an application programing
               interface (API) inspired by pandas with the Common Data Model
               for self-described scientific data. Key features of the xarray
               package include label-based indexing and arithmetic,
               interoperability with the core scientific Python packages (e.g.,
               pandas, NumPy, Matplotlib), out-of-core computation on datasets
               that don't fit into memory, a wide range of serialization and
               input/output (I/O) options, and advanced multi-dimensional data
               manipulation tools such as group-by and resampling. xarray, as a
               data model and analytics toolkit, has been widely adopted in the
               geoscience community but is also used more broadly for
               multi-dimensional data analysis in physics, machine learning and
               finance.",
  journal   = "J. Open Res. Softw.",
  publisher = "Ubiquity Press, Ltd.",
  volume    =  5,
  number    =  1,
  pages     = "10",
  month     =  apr,
  year      =  2017,
  keywords  = "Master thesis",
  copyright = "http://creativecommons.org/licenses/by/4.0",
  language  = "en"
}
